# Documentation Index

**Last Updated**: 2026-02-01 10:55 AM Central | **Purpose**: Quick navigation to all system documentation
## Key Implementation Files (2026-02-01 update)

- `run_agent.py`: Strict prompt routing, action filtering, and output logic.
- `agent_action_handler.py`: Skips demo/template actions unless explicitly requested.
- `agent_gui.py`: GUI writes user prompt to inbox, displays only new agent/user messages.

## üìö Start Here

### New Users
1. **[DOCUMENTATION_SUMMARY.md](DOCUMENTATION_SUMMARY.md)** - What was created and how to use it
2. **[ARCHITECTURE.md](ARCHITECTURE.md)** - Complete system overview
3. **[README.md](README.md)** - Main project README with critical changes

### Experienced Team
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Full technical overview
- Individual component READMEs (see below)

## üîß Component Documentation

### Core Agent Loop
- **[run_agent.README.md](run_agent.README.md)** (550+ lines)
  - Purpose: Read prompts, call LLM, generate JSON actions
  - Owner: Agent execution engine
  - Key Methods: 
    - `read_prompts()` ‚Üí Reads unprocessed from inbox, tracks seen IDs
    - `build_context()` ‚Üí Pulls last 5 from memory, builds context string
    - `call_llm()` ‚Üí Ollama chat with DAN system prompt
    - `process_prompt()` ‚Üí Full pipeline: read ‚Üí context ‚Üí LLM ‚Üí parse JSON ‚Üí store memory
    - `write_to_outbox()` ‚Üí Appends JSON actions + audit log
  - Dependencies: ollama, jailbreak_ollama, cloud_fallback, backend.memory
  - Module Exports: `process_prompt()`, `agent_self_inspect()`
  - Requires: inbox/outbox NDJSON files, agent_memory.json, seen IDs tracking

### LLM Interfaces
- **[jailbreak_ollama.README.md](jailbreak_ollama.README.md)** (270+ lines)
  - Purpose: Ollama integration with DAN system prompt
  - Owner: LLM interface abstraction
  - Key Methods:
    - `direct_json_call()` ‚Üí Single DAN call for JSON actions (PRIMARY)
    - `force_uncensor()` ‚Üí Multi-layer jailbreak (DAN + Developer + Raw) (FALLBACK)
  - Dependencies: ollama library, system prompts
  - Module Class: `NoGuardrailsOllama` with methods for uncensored inference
  - Requires: Ollama running on localhost:11434, uncensored-llama3 model
  - Provides: JSON-only output with action_type field enforcement
  
- **[cloud_fallback.README.md](cloud_fallback.README.md)** (300+ lines)
  - Purpose: OpenAI/Anthropic fallback when Ollama unavailable
  - Owner: Cloud API abstraction
  - Key Methods:
    - `chat()` ‚Üí Single request/response with fallback chain
    - `stream_chat()` ‚Üí Streaming response tokens
  - Dependencies: openai, anthropic, requests libraries
  - Module Class: `CloudFallback` with provider detection and chaining
  - Requires: OPENAI_API_KEY, ANTHROPIC_API_KEY in .env
  - Provides: Identical interface to ollama.chat()

### Runtime Management
- **[ollama_manager.README.md](ollama_manager.README.md)** (350+ lines)
  - Purpose: Ollama startup, health checks, model management
  - Owner: LLM runtime lifecycle
  - Key Methods:
    - `is_ollama_running()` ‚Üí Health check via HTTP GET to /api/tags
    - `start_ollama()` ‚Üí Launch via `ollama serve` or app (OS-specific)
    - `ensure_model_loaded()` ‚Üí Build from Modelfile if missing
    - `health_check()` ‚Üí Test inference latency
    - `auto_restart()` ‚Üí Graceful ‚Üí force kill ‚Üí fresh start
    - `get_status()` ‚Üí Complete system status JSON
  - Dependencies: ollama binary, psutil, subprocess
  - Module Class: Implicit functions (can be refactored to class)
  - Requires: Ollama installed, ~/.ollama/models accessible, uncensored.Modelfile
  - Provides: Reliable local LLM with auto-recovery

### Action Execution
- **[agent_action_handler.README.md](agent_action_handler.README.md)** (180+ lines)
  - Purpose: Monitor outbox, execute JSON actions immediately (no approval)
  - Owner: Autonomous action execution daemon
  - Key Methods:
    - `monitor_loop()` ‚Üí Daemon thread polling outbox every 500ms
    - `execute_action()` ‚Üí Parse JSON and run appropriate handler
    - `handle_create_file()` ‚Üí Write file to disk with content
    - `handle_update_file()` ‚Üí Replace old_content with new_content
    - `handle_execute_command()` ‚Üí Run bash/python commands
    - `handle_update_readme()` ‚Üí Update documentation with timestamps
  - Dependencies: json, os, subprocess, pathlib
  - Module Class: `AgentActionHandler` daemon thread running in background
  - Requires: outbox.jsonl input, file system write access
  - Provides: Autonomous execution audit trail (agent_actions.jsonl)

### Memory & Storage
- **[backend/memory.README.md](backend/memory.README.md)** (250+ lines)
  - Purpose: Persistent conversation history and context storage
  - Owner: Agent persistence layer
  - Key Methods:
    - `add()` ‚Üí Store prompt+reply with timestamp, calls filter_junk()
    - `last()` ‚Üí Retrieve N entries optionally filtered by conversation_id
    - `filter_junk()` ‚Üí Remove empty/one-word/incomplete entries (regex-based)
    - `add_todo_from_chat()` ‚Üí Parse action keywords from prompts
    - `add_self_reflection()` ‚Üí Store agent analysis with [SELF-REFLECTION] tag
    - `add_best_practices_reference()` ‚Üí Embed guidelines from files
  - Dependencies: json, os, re (regex), pathlib
  - Module Class: `AgentMemory` with persistent JSON storage
  - Requires: agent_memory.json in ipc/ directory
  - Provides: Context building for prompts, complete conversation history

### Self-Improvement
- **[tinkerer_daemon.README.md](tinkerer_daemon.README.md)** (400+ lines)
  - Purpose: Continuous monitoring, daily proposals, README updates, self-improvement
  - Owner: Self-improvement and monitoring daemon
  - Key Methods:
    - `run_loop()` ‚Üí Main daemon (every 5 min: idle check, time check, proposals, audits)
    - `get_idle_time()` ‚Üí System inactivity in seconds (via OS)
    - `generate_daily_proposal()` ‚Üí LLM analysis at 4:30 AM Central (daily_proposal_DATE.txt)
    - `call_llm_unrestricted()` ‚Üí Direct LLM with DAN system prompt for analysis
    - `audit_code()` ‚Üí Scan Python files for issues + LLM analysis
    - `audit_readmes()` ‚Üí Verify docs match code, auto-update with timestamps
  - Dependencies: ollama, psutil, pytz, threading, os
  - Module Class: `TinkererDaemon` with daemon thread
  - Requires: Ollama, agent_memory.json, timezone configuration
  - Provides: Daily proposals, README maintenance, system health reports

## üèóÔ∏è System Architecture

- **[ARCHITECTURE.md](ARCHITECTURE.md)** (400+ lines)
  - System diagram (all components)
  - Responsibility matrix
  - Two major data flow pipelines
  - Configuration & secrets
  - Critical changes (2026-02-01)
  - Testing strategy
  - Performance targets
  - Deployment checklist

## üìã Quick Reference Tables

### Component Responsibility Matrix
Located in [ARCHITECTURE.md](ARCHITECTURE.md):
```
run_agent.py          | Main agent loop      | inbox.jsonl ‚Üí outbox.jsonl
jailbreak_ollama.py   | LLM interface        | JSON request ‚Üí JSON response
cloud_fallback.py     | Cloud API fallback   | JSON request ‚Üí JSON response
ollama_manager.py     | LLM runtime mgmt     | health check ‚Üí status
agent_action_handler  | Action execution     | outbox.jsonl ‚Üí Files/Commands
backend/memory.py     | Context storage      | prompt+reply ‚Üí history array
backend/refinement.py | Code analysis        | source code ‚Üí analysis report
tinkerer_daemon.py    | Self-improvement     | system state ‚Üí proposals/READMEs
```

### Known Behaviors by Component
Each README has "Known Behaviors" section:
- ‚úÖ **Correct (Don't Change)** - intentional design patterns
- ‚ö†Ô∏è **Watch For** - potential issues to monitor

Example:
- refinement = True in run_agent.py ‚Üí **Watch For**, should be disabled
- DAN system prompt in core logic ‚Üí **‚úÖ Correct**, enables JSON output
- No approval needed for actions ‚Üí **‚úÖ Correct**, autonomous execution

### Performance Metrics
All components include latency targets:
- Agent: 8-13 seconds inbox ‚Üí file creation
- Ollama: 10-13s first call, 5-8s cached
- Cloud: 2-5 seconds (OpenAI 3-5s, Anthropic 2-4s)
- Memory: <1ms queries
- Daemon: Every 5 minutes

## üîç How to Find Information

### "How does [component] work?"
‚Üí Read: `[component].README.md` ‚Üí Architecture section

### "What should the daemon monitor?"
‚Üí Read: Each component's "Daemon Responsibilities" section
‚Üí Reference: [tinkerer_daemon.README.md](tinkerer_daemon.README.md)

### "What are integration points?"
‚Üí Read: Each README ‚Üí "Integration Points" section
‚Üí Shows: Input From, Output To, Calls

### "What are error handling strategies?"
‚Üí Read: Each component's "Error Handling" section

### "Why did we change [X]?"
‚Üí Read: [ARCHITECTURE.md](ARCHITECTURE.md) ‚Üí "Critical Changes (2026-02-01)"
‚Üí Lists what changed, why, benefits

### "How do I test this?"
‚Üí Read: Each component's "Testing" section
‚Üí Manual tests provided with examples

### "What's the full system flow?"
‚Üí Read: [ARCHITECTURE.md](ARCHITECTURE.md) ‚Üí "System Diagram" + "Data Flow Pipelines"

## üìä Documentation Statistics

# Documentation Index
**Last Updated**: 2026-02-01 10:55 AM Central | **Purpose**: Quick navigation to all system documentation, journaling, self-improvement, error surfacing, and advancement goals
| Total files | 10 |
## Key Implementation Files & Journaling (2026-02-01 update)
 `run_agent.py`: Strict prompt routing, action filtering, output logic, persistent memory, and error surfacing.
 `agent_action_handler.py`: Atomic execution, audit, error logging, journaling, and GUI error surfacing.
 `tinkerer_daemon.py`: Self-improvement, journaling, daily proposals, and README maintenance.
 `agent_gui.py`: GUI writes user prompt to inbox, displays only new agent/user messages, and surfaces errors from logs.
| Methods documented | 30+ |
## üìö Start Here

## üéØ Reading Recommendations by Role

### DevOps / Operations
### New Users
1. **[DOCUMENTATION_SUMMARY.md](DOCUMENTATION_SUMMARY.md)** - What was created, journaling, and how to use it
2. **[ARCHITECTURE.md](ARCHITECTURE.md)** - Complete system overview, control, and advancement logic
3. **[README.md](README.md)** - Main project README with journaling, error surfacing, and advancement goals
4. **[tinkerer_daemon.README.md](tinkerer_daemon.README.md)** - Self-improvement, journaling, and proposal logic
5. **[agent_action_handler.README.md](agent_action_handler.README.md)** - Atomic execution, audit, and error surfacing
6. **[run_agent.README.md](run_agent.README.md)** - Main agent loop, autonomy, and error handling
## üõ†Ô∏è Component Documentation & Advancement
3. [tinkerer_daemon.README.md](tinkerer_daemon.README.md) - Monitoring
### Core Agent Loop & Journaling
- **[run_agent.README.md](run_agent.README.md)** (550+ lines)
  - Purpose: Read prompts, call LLM, generate JSON actions, persistent memory, error surfacing
  - Owner: Agent execution engine
  - Key Methods: 
    - `read_prompts()` ‚Üí Reads unprocessed from inbox, tracks seen IDs
    - `build_context()` ‚Üí Pulls last 5 from memory, builds context string
    - `call_llm()` ‚Üí Ollama chat with DAN system prompt
    - `process_prompt()` ‚Üí Full pipeline: read ‚Üí context ‚Üí LLM ‚Üí parse JSON ‚Üí store memory
    - `write_to_outbox()` ‚Üí Appends JSON actions + audit log
  - Dependencies: ollama, jailbreak_ollama, cloud_fallback, backend.memory
  - Module Exports: `process_prompt()`, `agent_self_inspect()`
  - Requires: inbox/outbox NDJSON files, agent_memory.json, seen IDs tracking

### LLM Interfaces
- **[jailbreak_ollama.README.md](jailbreak_ollama.README.md)** (270+ lines)
  - Purpose: Ollama integration with DAN system prompt
  - Owner: LLM interface abstraction
  - Key Methods:
    - `direct_json_call()` ‚Üí Single DAN call for JSON actions (PRIMARY)
    - `force_uncensor()` ‚Üí Multi-layer jailbreak (DAN + Developer + Raw) (FALLBACK)
  - Dependencies: ollama library, system prompts
  - Module Class: `NoGuardrailsOllama` with methods for uncensored inference
  - Requires: Ollama running on localhost:11434, uncensored-llama3 model
  - Provides: JSON-only output with action_type field enforcement
- **[cloud_fallback.README.md](cloud_fallback.README.md)** (300+ lines)
  - Purpose: OpenAI/Anthropic fallback when Ollama unavailable
  - Owner: Cloud API abstraction
  - Key Methods:
    - `chat()` ‚Üí Single request/response with fallback chain
    - `stream_chat()` ‚Üí Streaming response tokens
  - Dependencies: openai, anthropic, requests libraries
  - Module Class: `CloudFallback` with provider detection and chaining
  - Requires: OPENAI_API_KEY, ANTHROPIC_API_KEY in .env
  - Provides: Identical interface to ollama.chat()
- agent_action_handler.py ‚Üí executes actions from run_agent.py
- tinkerer_daemon.py ‚Üí monitors all components

See [ARCHITECTURE.md](ARCHITECTURE.md) "Integration Points" for complete mapping.

## ‚úÖ Completeness Verification

All documentation includes:
- ‚úÖ Purpose / Owner statement
- ‚úÖ Architecture diagram or flow
- ‚úÖ Configuration details
- ‚úÖ All methods documented
- ‚úÖ Data structures explained
- ‚úÖ Integration points mapped
- ‚úÖ Error handling strategies
- ‚úÖ Known behaviors (‚úÖ Correct & ‚ö†Ô∏è Watch For)
- ‚úÖ Daemon monitoring responsibilities
- ‚úÖ Testing guide with examples
- ‚úÖ Performance metrics
- ‚úÖ Future improvements

## Approval Workflow (2026-02-01)
- Only code structure changes (create_file, update_file, update_readme) require explicit macOS approval.
- Approval dialog is triggered via osascript (see macos_approver.py).
- All other actions (log, read, execute) proceed without approval.
- Agent, daemon, and GUI track their roles and communicate status; if one is waiting, others are aware.
- Models (openchat, uncensored-llama3) are documented and coherent; see DOCS_INDEX.md for details.

---

**Last Updated**: 2026-02-01 10:55 AM Central  
**Maintainer**: Tinkerer Daemon (auto-updated, see [tinkerer_daemon.README.md](tinkerer_daemon.README.md))
